{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import time\n",
    "from pandas.plotting import lag_plot\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from math import sqrt\n",
    "from numpy import hstack\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import metrics\n",
    "\n",
    "data = pd.read_csv('/Users/alket/Desktop/dati/new_data_backfill_forwfill.csv',index_col = 0)\n",
    "# preparazione dati per due celle\n",
    "agg_by_cell = data.groupby(by = ['cell_num'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = []\n",
    "## prepare dataset\n",
    "for i, k in agg_by_cell:\n",
    "    cell_i = agg_by_cell.get_group(i)\n",
    "    # define input sequence\n",
    "    series_i = cell_i['nr_people'].values\n",
    "    series_i = series_i.reshape((len(series_i), 1))\n",
    "    num_data.append(series_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11808, 221)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data = hstack((num_data))\n",
    "final_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a multivariate sequence into samples\n",
    "def split_sequences(sequences, n_steps_in, n_steps_out):\n",
    "    X, y = list(), list()\n",
    "    for i in range(len(sequences)):\n",
    "        # find the end of this pattern\n",
    "        end_ix = i + n_steps_in\n",
    "        out_end_ix = end_ix + n_steps_out\n",
    "        # check if we are beyond the dataset\n",
    "        if out_end_ix > len(sequences):\n",
    "            break\n",
    "        # gather input and output parts of the pattern\n",
    "        seq_x, seq_y = sequences[i:end_ix, :], sequences[end_ix:out_end_ix, :]\n",
    "        X.append(seq_x)\n",
    "        y.append(seq_y)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# choose a number of time steps\n",
    "n_steps_in, n_steps_out = 4, 6\n",
    "\n",
    "# convert into input/output\n",
    "X, y = split_sequences(final_data, n_steps_in, n_steps_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  36.   39.   41.   41.   43.   44.   42.   41.   41.   41.   40.  446.\n",
      "   373.  219.   91.   24.   25.   39.   43.   42.   43.   43.   50.   53.\n",
      "    46.   41.   41.   69.  500.  382.  252.  173.   26.  124.  194.  205.\n",
      "    84.   43.   43.   45.   70.   57.   56.   50.   47.  312.  394.  357.\n",
      "   263.   78.  159.  320.  336.  203.  175.   68.   43.   67.   62.   58.\n",
      "    58.   56.   44.   83.  147.  229.  109.  150.  187.   74.  345.  103.\n",
      "   343.  122.   66.   56.   57.   58.   53.   42.   28.   23.   44.   80.\n",
      "   143.   47.   41.   93.  216.  158.  181. 1166. 1306.  345.   40.   55.\n",
      "    30.   25.   25.   24.   21.  213.   63.   66.  164.  218.  224.  145.\n",
      "   652.  205.  426.  270.   48.   26.   24.   25.   24.   43.  233.   23.\n",
      "    72.  261.   99.   91.   55.  208.  103.  614.  202.   23.   25.   24.\n",
      "    24.   22.   86.  201.   65.   43.  136.  210.  136.  171.   82.   75.\n",
      "    42.   25.   23.   23.   24.   24.   18.  149.  415.   26.   29.   35.\n",
      "    96.  238.  256.  295.  170.  174.  352.  494.  488.  472.  398.  326.\n",
      "   330.  151.  344.  310.  204.  357.  315.  265.  255.  273.  199.   30.\n",
      "    20.   19.   18.   18.   19.   17.   36.   57.   17.  365.  198.   51.\n",
      "   123.  129.  178.  130.   28.   18.   17.   18.   17.   17.   17.  105.\n",
      "   141.  176.  190.   13.   72.  135.  131.  153.  178.   89.   17.   17.\n",
      "    17.   17.   17.  104.   35.]\n",
      " [  36.   38.   40.   41.   43.   44.   42.   41.   41.   41.   40.  449.\n",
      "   368.  217.   90.   24.   25.   39.   42.   42.   42.   43.   51.   54.\n",
      "    46.   41.   41.   69.  503.  381.  250.  172.   26.  124.  191.  201.\n",
      "    83.   42.   42.   46.   71.   58.   56.   51.   47.  313.  396.  356.\n",
      "   262.   78.  160.  315.  329.  199.  172.   66.   43.   68.   63.   58.\n",
      "    59.   57.   44.   82.  146.  228.  109.  151.  185.   73.  338.  101.\n",
      "   336.  120.   67.   57.   57.   58.   54.   42.   28.   23.   43.   81.\n",
      "   144.   47.   40.   91.  211.  155.  177. 1191. 1332.  350.   40.   57.\n",
      "    31.   25.   25.   24.   21.  214.   63.   64.  161.  214.  219.  139.\n",
      "   645.  208.  438.  279.   50.   27.   24.   25.   24.   43.  233.   23.\n",
      "    71.  256.   97.   89.   52.  200.  106.  634.  208.   23.   26.   24.\n",
      "    24.   22.   87.  200.   65.   43.  135.  206.  129.  160.   79.   74.\n",
      "    43.   26.   23.   23.   25.   24.   19.  151.  419.   26.   29.   35.\n",
      "    95.  232.  248.  288.  166.  173.  353.  497.  490.  478.  403.  330.\n",
      "   335.  153.  342.  309.  203.  357.  311.  258.  249.  267.  198.   30.\n",
      "    20.   20.   18.   19.   19.   18.   36.   57.   17.  364.  198.   52.\n",
      "   122.  127.  176.  130.   28.   18.   17.   18.   17.   17.   17.  107.\n",
      "   141.  176.  190.   13.   72.  135.  132.  154.  181.   91.   17.   17.\n",
      "    17.   17.   17.  106.   36.]\n",
      " [  34.   36.   38.   38.   40.   42.   40.   40.   40.   40.   39.  437.\n",
      "   363.  214.   89.   24.   24.   36.   39.   39.   39.   40.   47.   51.\n",
      "    45.   40.   40.   67.  489.  372.  247.  170.   26.  122.  179.  189.\n",
      "    78.   39.   39.   43.   67.   55.   54.   49.   46.  305.  387.  350.\n",
      "   259.   78.  158.  296.  309.  187.  162.   63.   40.   64.   60.   56.\n",
      "    57.   54.   43.   82.  144.  224.  109.  149.  174.   69.  318.   95.\n",
      "   317.  113.   64.   54.   55.   56.   52.   41.   28.   23.   43.   80.\n",
      "   143.   44.   38.   86.  199.  146.  167. 1129. 1268.  334.   38.   54.\n",
      "    29.   24.   25.   24.   21.  214.   59.   61.  152.  201.  207.  134.\n",
      "   618.  198.  411.  260.   47.   26.   24.   25.   24.   42.  234.   22.\n",
      "    68.  242.   91.   85.   51.  193.   98.  588.  194.   22.   24.   23.\n",
      "    24.   22.   85.  199.   65.   41.  128.  196.  127.  159.   76.   70.\n",
      "    40.   25.   22.   22.   23.   23.   19.  149.  419.   26.   29.   35.\n",
      "    93.  231.  247.  283.  163.  172.  341.  482.  476.  467.  395.  321.\n",
      "   333.  154.  343.  310.  204.  358.  312.  257.  246.  263.  195.   29.\n",
      "    19.   19.   18.   18.   19.   18.   36.   57.   17.  364.  199.   52.\n",
      "   122.  127.  173.  126.   28.   18.   17.   17.   17.   17.   17.  107.\n",
      "   141.  176.  189.   13.   72.  135.  131.  151.  177.   89.   17.   17.\n",
      "    16.   17.   17.  107.   36.]\n",
      " [  33.   35.   37.   37.   39.   40.   39.   38.   38.   38.   38.  420.\n",
      "   351.  209.   87.   23.   24.   36.   38.   38.   38.   39.   46.   50.\n",
      "    43.   39.   38.   65.  470.  359.  241.  166.   25.  120.  175.  184.\n",
      "    76.   38.   38.   41.   66.   54.   53.   47.   45.  294.  375.  339.\n",
      "   254.   76.  156.  289.  301.  182.  157.   61.   39.   63.   58.   55.\n",
      "    55.   53.   42.   80.  141.  220.  108.  147.  171.   67.  310.   93.\n",
      "   307.  109.   62.   53.   53.   55.   50.   40.   27.   22.   42.   80.\n",
      "   141.   43.   37.   83.  193.  141.  162. 1091. 1234.  326.   37.   52.\n",
      "    28.   24.   24.   24.   21.  210.   59.   59.  148.  196.  201.  129.\n",
      "   599.  193.  400.  254.   46.   25.   23.   25.   24.   42.  232.   22.\n",
      "    67.  236.   89.   83.   49.  187.   96.  575.  189.   22.   24.   23.\n",
      "    24.   22.   85.  200.   66.   41.  126.  192.  124.  156.   75.   70.\n",
      "    40.   25.   22.   22.   23.   23.   18.  149.  419.   26.   29.   34.\n",
      "    93.  230.  246.  284.  164.  171.  339.  480.  476.  466.  394.  320.\n",
      "   331.  153.  345.  312.  205.  359.  310.  257.  247.  264.  195.   29.\n",
      "    20.   19.   18.   18.   19.   17.   36.   57.   17.  365.  198.   52.\n",
      "   121.  127.  173.  127.   28.   18.   17.   17.   17.   17.   17.  106.\n",
      "   141.  176.  188.   13.   72.  134.  130.  150.  177.   88.   17.   16.\n",
      "    16.   17.   17.  106.   35.]] [[ 32.  34.  36. ...  17. 105.  35.]\n",
      " [ 33.  35.  36. ...  17. 105.  35.]\n",
      " [ 32.  33.  35. ...  17. 105.  35.]\n",
      " [ 30.  32.  33. ...  17. 105.  35.]\n",
      " [ 30.  32.  33. ...  17. 105.  35.]\n",
      " [ 29.  31.  32. ...  17. 105.  35.]]\n",
      "[[  36.   38.   40.   41.   43.   44.   42.   41.   41.   41.   40.  449.\n",
      "   368.  217.   90.   24.   25.   39.   42.   42.   42.   43.   51.   54.\n",
      "    46.   41.   41.   69.  503.  381.  250.  172.   26.  124.  191.  201.\n",
      "    83.   42.   42.   46.   71.   58.   56.   51.   47.  313.  396.  356.\n",
      "   262.   78.  160.  315.  329.  199.  172.   66.   43.   68.   63.   58.\n",
      "    59.   57.   44.   82.  146.  228.  109.  151.  185.   73.  338.  101.\n",
      "   336.  120.   67.   57.   57.   58.   54.   42.   28.   23.   43.   81.\n",
      "   144.   47.   40.   91.  211.  155.  177. 1191. 1332.  350.   40.   57.\n",
      "    31.   25.   25.   24.   21.  214.   63.   64.  161.  214.  219.  139.\n",
      "   645.  208.  438.  279.   50.   27.   24.   25.   24.   43.  233.   23.\n",
      "    71.  256.   97.   89.   52.  200.  106.  634.  208.   23.   26.   24.\n",
      "    24.   22.   87.  200.   65.   43.  135.  206.  129.  160.   79.   74.\n",
      "    43.   26.   23.   23.   25.   24.   19.  151.  419.   26.   29.   35.\n",
      "    95.  232.  248.  288.  166.  173.  353.  497.  490.  478.  403.  330.\n",
      "   335.  153.  342.  309.  203.  357.  311.  258.  249.  267.  198.   30.\n",
      "    20.   20.   18.   19.   19.   18.   36.   57.   17.  364.  198.   52.\n",
      "   122.  127.  176.  130.   28.   18.   17.   18.   17.   17.   17.  107.\n",
      "   141.  176.  190.   13.   72.  135.  132.  154.  181.   91.   17.   17.\n",
      "    17.   17.   17.  106.   36.]\n",
      " [  34.   36.   38.   38.   40.   42.   40.   40.   40.   40.   39.  437.\n",
      "   363.  214.   89.   24.   24.   36.   39.   39.   39.   40.   47.   51.\n",
      "    45.   40.   40.   67.  489.  372.  247.  170.   26.  122.  179.  189.\n",
      "    78.   39.   39.   43.   67.   55.   54.   49.   46.  305.  387.  350.\n",
      "   259.   78.  158.  296.  309.  187.  162.   63.   40.   64.   60.   56.\n",
      "    57.   54.   43.   82.  144.  224.  109.  149.  174.   69.  318.   95.\n",
      "   317.  113.   64.   54.   55.   56.   52.   41.   28.   23.   43.   80.\n",
      "   143.   44.   38.   86.  199.  146.  167. 1129. 1268.  334.   38.   54.\n",
      "    29.   24.   25.   24.   21.  214.   59.   61.  152.  201.  207.  134.\n",
      "   618.  198.  411.  260.   47.   26.   24.   25.   24.   42.  234.   22.\n",
      "    68.  242.   91.   85.   51.  193.   98.  588.  194.   22.   24.   23.\n",
      "    24.   22.   85.  199.   65.   41.  128.  196.  127.  159.   76.   70.\n",
      "    40.   25.   22.   22.   23.   23.   19.  149.  419.   26.   29.   35.\n",
      "    93.  231.  247.  283.  163.  172.  341.  482.  476.  467.  395.  321.\n",
      "   333.  154.  343.  310.  204.  358.  312.  257.  246.  263.  195.   29.\n",
      "    19.   19.   18.   18.   19.   18.   36.   57.   17.  364.  199.   52.\n",
      "   122.  127.  173.  126.   28.   18.   17.   17.   17.   17.   17.  107.\n",
      "   141.  176.  189.   13.   72.  135.  131.  151.  177.   89.   17.   17.\n",
      "    16.   17.   17.  107.   36.]\n",
      " [  33.   35.   37.   37.   39.   40.   39.   38.   38.   38.   38.  420.\n",
      "   351.  209.   87.   23.   24.   36.   38.   38.   38.   39.   46.   50.\n",
      "    43.   39.   38.   65.  470.  359.  241.  166.   25.  120.  175.  184.\n",
      "    76.   38.   38.   41.   66.   54.   53.   47.   45.  294.  375.  339.\n",
      "   254.   76.  156.  289.  301.  182.  157.   61.   39.   63.   58.   55.\n",
      "    55.   53.   42.   80.  141.  220.  108.  147.  171.   67.  310.   93.\n",
      "   307.  109.   62.   53.   53.   55.   50.   40.   27.   22.   42.   80.\n",
      "   141.   43.   37.   83.  193.  141.  162. 1091. 1234.  326.   37.   52.\n",
      "    28.   24.   24.   24.   21.  210.   59.   59.  148.  196.  201.  129.\n",
      "   599.  193.  400.  254.   46.   25.   23.   25.   24.   42.  232.   22.\n",
      "    67.  236.   89.   83.   49.  187.   96.  575.  189.   22.   24.   23.\n",
      "    24.   22.   85.  200.   66.   41.  126.  192.  124.  156.   75.   70.\n",
      "    40.   25.   22.   22.   23.   23.   18.  149.  419.   26.   29.   34.\n",
      "    93.  230.  246.  284.  164.  171.  339.  480.  476.  466.  394.  320.\n",
      "   331.  153.  345.  312.  205.  359.  310.  257.  247.  264.  195.   29.\n",
      "    20.   19.   18.   18.   19.   17.   36.   57.   17.  365.  198.   52.\n",
      "   121.  127.  173.  127.   28.   18.   17.   17.   17.   17.   17.  106.\n",
      "   141.  176.  188.   13.   72.  134.  130.  150.  177.   88.   17.   16.\n",
      "    16.   17.   17.  106.   35.]\n",
      " [  32.   34.   36.   36.   37.   38.   37.   37.   37.   37.   36.  401.\n",
      "   343.  209.   87.   23.   24.   35.   37.   37.   37.   37.   44.   48.\n",
      "    42.   37.   37.   62.  450.  349.  240.  166.   25.  120.  170.  178.\n",
      "    73.   37.   37.   39.   63.   52.   51.   46.   43.  282.  365.  331.\n",
      "   253.   76.  157.  280.  292.  176.  152.   59.   37.   60.   56.   53.\n",
      "    53.   51.   40.   80.  141.  219.  108.  147.  167.   65.  300.   90.\n",
      "   298.  106.   59.   51.   52.   53.   49.   39.   27.   22.   42.   80.\n",
      "   141.   42.   36.   81.  187.  137.  157. 1033. 1180.  314.   36.   50.\n",
      "    27.   24.   24.   24.   21.  211.   57.   58.  143.  189.  195.  127.\n",
      "   581.  186.  379.  241.   44.   24.   23.   24.   24.   41.  233.   22.\n",
      "    65.  229.   86.   80.   48.  184.   91.  542.  180.   21.   23.   22.\n",
      "    23.   22.   85.  201.   66.   41.  123.  187.  121.  153.   75.   68.\n",
      "    39.   24.   22.   22.   21.   23.   18.  149.  420.   26.   29.   34.\n",
      "    93.  233.  249.  286.  165.  172.  336.  474.  469.  460.  391.  318.\n",
      "   330.  155.  346.  314.  206.  361.  314.  260.  250.  266.  195.   29.\n",
      "    19.   19.   17.   18.   19.   17.   36.   57.   17.  367.  201.   53.\n",
      "   123.  128.  174.  126.   28.   17.   17.   17.   17.   17.   16.  106.\n",
      "   141.  177.  191.   13.   73.  135.  131.  149.  175.   88.   17.   16.\n",
      "    16.   16.   17.  105.   35.]] [[ 33.  35.  36. ...  17. 105.  35.]\n",
      " [ 32.  33.  35. ...  17. 105.  35.]\n",
      " [ 30.  32.  33. ...  17. 105.  35.]\n",
      " [ 30.  32.  33. ...  17. 105.  35.]\n",
      " [ 29.  31.  32. ...  17. 105.  35.]\n",
      " [ 29.  30.  31. ...  17. 105.  35.]]\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in range(len(X)):\n",
    "    count +=1 \n",
    "    if count > 2: break\n",
    "    print(X[i], y[i])\n",
    "\n",
    "# flatten output\n",
    "n_output = y.shape[1] * y.shape[2]\n",
    "y = y.reshape((y.shape[0], n_output))\n",
    "\n",
    "# the dataset knows the number of features, e.g. 2\n",
    "n_features = X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8849 8849 2950 2950\n"
     ]
    }
   ],
   "source": [
    "split_train_test = int(len(X)*0.75)\n",
    "train_X, test_X = X[:split_train_test], X[split_train_test:]\n",
    "train_y, test_y = y[:split_train_test], y[split_train_test:]\n",
    "print(len(train_X), len(train_y), len(test_X), len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      " - 1s - loss: 4259.2672 - mean_absolute_error: 28.3491 - accuracy: 0.1359\n",
      "Epoch 2/100\n",
      " - 1s - loss: 651.1853 - mean_absolute_error: 13.5429 - accuracy: 0.1197\n",
      "Epoch 3/100\n",
      " - 1s - loss: 481.6444 - mean_absolute_error: 11.2038 - accuracy: 0.2734\n",
      "Epoch 4/100\n",
      " - 1s - loss: 415.7391 - mean_absolute_error: 10.3947 - accuracy: 0.3629\n",
      "Epoch 5/100\n",
      " - 1s - loss: 379.1226 - mean_absolute_error: 9.9377 - accuracy: 0.3710\n",
      "Epoch 6/100\n",
      " - 1s - loss: 358.6623 - mean_absolute_error: 9.6937 - accuracy: 0.3791\n",
      "Epoch 7/100\n",
      " - 1s - loss: 342.1478 - mean_absolute_error: 9.4799 - accuracy: 0.3947\n",
      "Epoch 8/100\n",
      " - 1s - loss: 323.4578 - mean_absolute_error: 9.0241 - accuracy: 0.4013\n",
      "Epoch 9/100\n",
      " - 1s - loss: 311.9332 - mean_absolute_error: 8.8558 - accuracy: 0.4134\n",
      "Epoch 10/100\n",
      " - 1s - loss: 315.3489 - mean_absolute_error: 8.9773 - accuracy: 0.4261\n",
      "Epoch 11/100\n",
      " - 1s - loss: 309.0908 - mean_absolute_error: 8.9125 - accuracy: 0.4371\n",
      "Epoch 12/100\n",
      " - 1s - loss: 294.8866 - mean_absolute_error: 8.6052 - accuracy: 0.4480\n",
      "Epoch 13/100\n",
      " - 1s - loss: 290.8957 - mean_absolute_error: 8.6531 - accuracy: 0.4575\n",
      "Epoch 14/100\n",
      " - 1s - loss: 288.5699 - mean_absolute_error: 8.5122 - accuracy: 0.4593\n",
      "Epoch 15/100\n",
      " - 1s - loss: 282.7184 - mean_absolute_error: 8.4434 - accuracy: 0.4672\n",
      "Epoch 16/100\n",
      " - 1s - loss: 281.9853 - mean_absolute_error: 8.3951 - accuracy: 0.4645\n",
      "Epoch 17/100\n",
      " - 1s - loss: 280.6595 - mean_absolute_error: 8.3828 - accuracy: 0.4703\n",
      "Epoch 18/100\n",
      " - 1s - loss: 286.3897 - mean_absolute_error: 8.5404 - accuracy: 0.4695\n",
      "Epoch 19/100\n",
      " - 1s - loss: 274.2268 - mean_absolute_error: 8.2595 - accuracy: 0.4663\n",
      "Epoch 20/100\n",
      " - 1s - loss: 275.9024 - mean_absolute_error: 8.2739 - accuracy: 0.4772\n",
      "Epoch 21/100\n",
      " - 1s - loss: 268.2124 - mean_absolute_error: 8.1363 - accuracy: 0.4758\n",
      "Epoch 22/100\n",
      " - 1s - loss: 276.4283 - mean_absolute_error: 8.3706 - accuracy: 0.4790\n",
      "Epoch 23/100\n",
      " - 1s - loss: 264.5752 - mean_absolute_error: 8.0582 - accuracy: 0.4837\n",
      "Epoch 24/100\n",
      " - 1s - loss: 276.0559 - mean_absolute_error: 8.3517 - accuracy: 0.4822\n",
      "Epoch 25/100\n",
      " - 1s - loss: 266.4034 - mean_absolute_error: 8.1171 - accuracy: 0.4849\n",
      "Epoch 26/100\n",
      " - 1s - loss: 260.2076 - mean_absolute_error: 7.9818 - accuracy: 0.4880\n",
      "Epoch 27/100\n",
      " - 1s - loss: 264.3052 - mean_absolute_error: 8.1188 - accuracy: 0.4911\n",
      "Epoch 28/100\n",
      " - 1s - loss: 266.4517 - mean_absolute_error: 8.2759 - accuracy: 0.4926\n",
      "Epoch 29/100\n",
      " - 1s - loss: 265.2070 - mean_absolute_error: 8.1365 - accuracy: 0.4952\n",
      "Epoch 30/100\n",
      " - 1s - loss: 270.0646 - mean_absolute_error: 8.2788 - accuracy: 0.5003\n",
      "Epoch 31/100\n",
      " - 1s - loss: 258.8409 - mean_absolute_error: 7.9865 - accuracy: 0.5023\n",
      "Epoch 32/100\n",
      " - 1s - loss: 256.9044 - mean_absolute_error: 7.9665 - accuracy: 0.5013\n",
      "Epoch 33/100\n",
      " - 1s - loss: 255.2375 - mean_absolute_error: 7.9083 - accuracy: 0.5036\n",
      "Epoch 34/100\n",
      " - 1s - loss: 262.0688 - mean_absolute_error: 8.0394 - accuracy: 0.5031\n",
      "Epoch 35/100\n",
      " - 1s - loss: 261.9426 - mean_absolute_error: 8.0621 - accuracy: 0.5039\n",
      "Epoch 36/100\n",
      " - 1s - loss: 256.3423 - mean_absolute_error: 7.9280 - accuracy: 0.4997\n",
      "Epoch 37/100\n",
      " - 1s - loss: 256.5606 - mean_absolute_error: 7.9442 - accuracy: 0.5053\n",
      "Epoch 38/100\n",
      " - 1s - loss: 255.1409 - mean_absolute_error: 7.9024 - accuracy: 0.5083\n",
      "Epoch 39/100\n",
      " - 1s - loss: 250.5004 - mean_absolute_error: 7.8319 - accuracy: 0.5079\n",
      "Epoch 40/100\n",
      " - 1s - loss: 248.1535 - mean_absolute_error: 7.7770 - accuracy: 0.5083\n",
      "Epoch 41/100\n",
      " - 1s - loss: 246.8109 - mean_absolute_error: 7.7543 - accuracy: 0.5068\n",
      "Epoch 42/100\n",
      " - 1s - loss: 246.6458 - mean_absolute_error: 7.7361 - accuracy: 0.5066\n",
      "Epoch 43/100\n",
      " - 1s - loss: 254.7618 - mean_absolute_error: 7.9225 - accuracy: 0.5075\n",
      "Epoch 44/100\n",
      " - 1s - loss: 245.1018 - mean_absolute_error: 7.7229 - accuracy: 0.5085\n",
      "Epoch 45/100\n",
      " - 1s - loss: 242.7016 - mean_absolute_error: 7.6202 - accuracy: 0.5126\n",
      "Epoch 46/100\n",
      " - 1s - loss: 257.5521 - mean_absolute_error: 7.9898 - accuracy: 0.5047\n",
      "Epoch 47/100\n",
      " - 1s - loss: 245.9534 - mean_absolute_error: 7.7700 - accuracy: 0.5092\n",
      "Epoch 48/100\n",
      " - 1s - loss: 247.9889 - mean_absolute_error: 7.7901 - accuracy: 0.5133\n",
      "Epoch 49/100\n",
      " - 1s - loss: 239.9326 - mean_absolute_error: 7.6143 - accuracy: 0.5081\n",
      "Epoch 50/100\n",
      " - 1s - loss: 245.9857 - mean_absolute_error: 7.7147 - accuracy: 0.5054\n",
      "Epoch 51/100\n",
      " - 1s - loss: 239.4339 - mean_absolute_error: 7.5879 - accuracy: 0.5111\n",
      "Epoch 52/100\n",
      " - 1s - loss: 248.2631 - mean_absolute_error: 7.7996 - accuracy: 0.5116\n",
      "Epoch 53/100\n",
      " - 1s - loss: 236.6689 - mean_absolute_error: 7.5171 - accuracy: 0.5111\n",
      "Epoch 54/100\n",
      " - 1s - loss: 237.3127 - mean_absolute_error: 7.5580 - accuracy: 0.5115\n",
      "Epoch 55/100\n",
      " - 1s - loss: 236.9987 - mean_absolute_error: 7.5844 - accuracy: 0.5110\n",
      "Epoch 56/100\n",
      " - 1s - loss: 235.2346 - mean_absolute_error: 7.4989 - accuracy: 0.5107\n",
      "Epoch 57/100\n",
      " - 1s - loss: 234.9831 - mean_absolute_error: 7.5265 - accuracy: 0.5145\n",
      "Epoch 58/100\n",
      " - 1s - loss: 238.9638 - mean_absolute_error: 7.6086 - accuracy: 0.5137\n",
      "Epoch 59/100\n",
      " - 1s - loss: 233.2825 - mean_absolute_error: 7.4709 - accuracy: 0.5095\n",
      "Epoch 60/100\n",
      " - 1s - loss: 236.3765 - mean_absolute_error: 7.5774 - accuracy: 0.5112\n",
      "Epoch 61/100\n",
      " - 1s - loss: 233.1014 - mean_absolute_error: 7.5138 - accuracy: 0.5137\n",
      "Epoch 62/100\n",
      " - 1s - loss: 237.8295 - mean_absolute_error: 7.5967 - accuracy: 0.5085\n",
      "Epoch 63/100\n",
      " - 1s - loss: 233.4756 - mean_absolute_error: 7.4994 - accuracy: 0.5066\n",
      "Epoch 64/100\n",
      " - 1s - loss: 229.6584 - mean_absolute_error: 7.4064 - accuracy: 0.5095\n",
      "Epoch 65/100\n",
      " - 1s - loss: 239.0252 - mean_absolute_error: 7.6812 - accuracy: 0.5075\n",
      "Epoch 66/100\n",
      " - 1s - loss: 230.5891 - mean_absolute_error: 7.4472 - accuracy: 0.5106\n",
      "Epoch 67/100\n",
      " - 1s - loss: 230.5793 - mean_absolute_error: 7.4444 - accuracy: 0.5116\n",
      "Epoch 68/100\n",
      " - 1s - loss: 237.5073 - mean_absolute_error: 7.6586 - accuracy: 0.5145\n",
      "Epoch 69/100\n",
      " - 1s - loss: 225.1088 - mean_absolute_error: 7.3228 - accuracy: 0.5149\n",
      "Epoch 70/100\n",
      " - 1s - loss: 229.9524 - mean_absolute_error: 7.5011 - accuracy: 0.5105\n",
      "Epoch 71/100\n",
      " - 1s - loss: 226.4438 - mean_absolute_error: 7.3651 - accuracy: 0.5109\n",
      "Epoch 72/100\n",
      " - 1s - loss: 224.3308 - mean_absolute_error: 7.3105 - accuracy: 0.5100\n",
      "Epoch 73/100\n",
      " - 1s - loss: 225.1962 - mean_absolute_error: 7.3694 - accuracy: 0.5109\n",
      "Epoch 74/100\n",
      " - 1s - loss: 228.0810 - mean_absolute_error: 7.4266 - accuracy: 0.5127\n",
      "Epoch 75/100\n",
      " - 1s - loss: 226.6083 - mean_absolute_error: 7.3846 - accuracy: 0.5158\n",
      "Epoch 76/100\n",
      " - 1s - loss: 225.2770 - mean_absolute_error: 7.3850 - accuracy: 0.5152\n",
      "Epoch 77/100\n",
      " - 1s - loss: 222.9940 - mean_absolute_error: 7.3010 - accuracy: 0.5150\n",
      "Epoch 78/100\n",
      " - 1s - loss: 219.8628 - mean_absolute_error: 7.2340 - accuracy: 0.5112\n",
      "Epoch 79/100\n",
      " - 1s - loss: 226.1107 - mean_absolute_error: 7.3826 - accuracy: 0.5129\n",
      "Epoch 80/100\n",
      " - 1s - loss: 223.6011 - mean_absolute_error: 7.3364 - accuracy: 0.5126\n",
      "Epoch 81/100\n",
      " - 1s - loss: 226.6025 - mean_absolute_error: 7.3985 - accuracy: 0.5127\n",
      "Epoch 82/100\n",
      " - 1s - loss: 223.8946 - mean_absolute_error: 7.3741 - accuracy: 0.5102\n",
      "Epoch 83/100\n",
      " - 1s - loss: 222.9739 - mean_absolute_error: 7.3011 - accuracy: 0.5124\n",
      "Epoch 84/100\n",
      " - 1s - loss: 222.3570 - mean_absolute_error: 7.3049 - accuracy: 0.5111\n",
      "Epoch 85/100\n",
      " - 1s - loss: 218.0784 - mean_absolute_error: 7.2574 - accuracy: 0.5176\n",
      "Epoch 86/100\n",
      " - 1s - loss: 218.6547 - mean_absolute_error: 7.2854 - accuracy: 0.5172\n",
      "Epoch 87/100\n",
      " - 1s - loss: 220.4787 - mean_absolute_error: 7.3230 - accuracy: 0.5137\n",
      "Epoch 88/100\n",
      " - 1s - loss: 213.2094 - mean_absolute_error: 7.1137 - accuracy: 0.5133\n",
      "Epoch 89/100\n",
      " - 1s - loss: 214.5098 - mean_absolute_error: 7.1983 - accuracy: 0.5163\n",
      "Epoch 90/100\n",
      " - 1s - loss: 221.3693 - mean_absolute_error: 7.3685 - accuracy: 0.5115\n",
      "Epoch 91/100\n",
      " - 1s - loss: 215.3814 - mean_absolute_error: 7.2110 - accuracy: 0.5140\n",
      "Epoch 92/100\n",
      " - 1s - loss: 215.0778 - mean_absolute_error: 7.2286 - accuracy: 0.5128\n",
      "Epoch 93/100\n",
      " - 1s - loss: 215.2221 - mean_absolute_error: 7.2560 - accuracy: 0.5167\n",
      "Epoch 94/100\n",
      " - 1s - loss: 206.5907 - mean_absolute_error: 7.0773 - accuracy: 0.5158\n",
      "Epoch 95/100\n",
      " - 1s - loss: 211.0058 - mean_absolute_error: 7.1482 - accuracy: 0.5167\n",
      "Epoch 96/100\n",
      " - 1s - loss: 208.7976 - mean_absolute_error: 7.1697 - accuracy: 0.5117\n",
      "Epoch 97/100\n",
      " - 1s - loss: 206.4588 - mean_absolute_error: 7.0608 - accuracy: 0.5138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98/100\n",
      " - 1s - loss: 209.5753 - mean_absolute_error: 7.1864 - accuracy: 0.5249\n",
      "Epoch 99/100\n",
      " - 1s - loss: 208.0199 - mean_absolute_error: 7.1233 - accuracy: 0.5215\n",
      "Epoch 100/100\n",
      " - 1s - loss: 206.7142 - mean_absolute_error: 7.1122 - accuracy: 0.5220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a3361f050>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(n_steps_in,\n",
    "n_features)))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(50, activation='relu')) \n",
    "model.add(Dense(n_output))\n",
    "model.compile(optimizer='adam', loss='mse',  metrics=[metrics.mae, 'accuracy'])\n",
    "# fit model\n",
    "model.fit(train_X, train_y, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and compute error\n",
    "predicted = np.array([])\n",
    "\n",
    "for i in range(len(test_X)):\n",
    "    x_input = test_X[i].reshape((1, n_steps_in, n_features))\n",
    "    yhat = model.predict(x_input, verbose=0)\n",
    "    #print(yhat[0])\n",
    "    predicted = np.append(predicted, yhat[0]) \n",
    "\n",
    "\n",
    "expected = test_y  \n",
    "expected = np.reshape(expected, expected.shape[1] * expected.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error =  8.46786448386152\n"
     ]
    }
   ],
   "source": [
    "# abs difference\n",
    "difference = abs((expected - predicted))\n",
    "print('Mean Absolute Error = ', np.mean(difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show boxplot\n",
    "plt.figure(figsize = (14,2))\n",
    "plt.boxplot(difference, vert= False);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
